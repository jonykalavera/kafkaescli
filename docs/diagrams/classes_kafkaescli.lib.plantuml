@startuml classes_kafkaescli.lib

class "AIOKafkaConsumer" as aiokafka.consumer.consumer.AIOKafkaConsumer #antiquewhite {
  assign(partitions)
  assignment()
  beginning_offsets(partitions)
  commit(offsets)
  committed(partition)
  end_offsets(partitions)
  getmany()
  getone()
  highwater(partition)
  last_poll_timestamp(partition)
  last_stable_offset(partition)
  offsets_for_times(timestamps)
  partitions_for_topic(topic)
  pause()
  paused()
  position(partition)
  resume()
  seek(partition, offset)
  seek_to_beginning()
  seek_to_committed()
  seek_to_end()
  start()
  stop()
  subscribe(topics, pattern, listener)
  subscription()
  topics()
  unsubscribe()
}
class "AIOKafkaProducer" as aiokafka.producer.producer.AIOKafkaProducer #aquamarine {
  client : AIOKafkaClient
  abort_transaction()
  begin_transaction()
  commit_transaction()
  create_batch()
  flush()
  partitions_for(topic)
  send(topic, value, key, partition, timestamp_ms, headers)
  send_and_wait(topic, value, key, partition, timestamp_ms, headers)
  send_batch(batch, topic)
  send_offsets_to_transaction(offsets, group_id)
  start()
  stop()
  transaction()
}
class "AsyncCommand" as kafkaescli.lib.commands.AsyncCommand #aliceblue {
  execute() -> Result[R, BaseException]
}
class "AsyncHookCallback" as kafkaescli.lib.middleware.AsyncHookCallback #aliceblue {
}
class "ClientSession" as aiohttp.client.ClientSession #burlywood {
  ATTRS : frozenset
  auth
  auto_decompress
  closed
  connector
  connector_owner
  cookie_jar
  headers
  json_serialize
  loop
  raise_for_status
  requote_redirect_url
  skip_auto_headers
  timeout
  trace_configs
  trust_env
  version
  close() -> None
  delete(url: StrOrURL) -> '_RequestContextManager'
  detach() -> None
  get(url: StrOrURL) -> '_RequestContextManager'
  head(url: StrOrURL) -> '_RequestContextManager'
  options(url: StrOrURL) -> '_RequestContextManager'
  patch(url: StrOrURL) -> '_RequestContextManager'
  post(url: StrOrURL) -> '_RequestContextManager'
  put(url: StrOrURL) -> '_RequestContextManager'
  request(method: str, url: StrOrURL) -> '_RequestContextManager'
  ws_connect(url: StrOrURL) -> '_WSRequestContextManager'
}
class "Command" as kafkaescli.lib.commands.Command #aliceblue {
  execute_async() -> Result[R, BaseException]
}
class "CommandInterface" as kafkaescli.lib.commands.CommandInterface #aliceblue {
  execute() -> Result[R, BaseException]
  execute_async() -> Result[R, BaseException]
}
class "Consumer" as kafkaescli.lib.kafka.Consumer #aliceblue {
  auto_offset_reset : str
  bootstrap_servers : str
  enable_auto_commit : bool
  group_id : Optional[str]
  topics : List[str]
  consumer_record_to_payload(value: 'ConsumerRecord') -> ConsumerPayload
  execute() -> AsyncIterator[ConsumerPayload]
}
class "MiddlewareHook" as kafkaescli.domain.models.MiddlewareHook #cadetblue {
  name
}
class "MiddlewarePipeline" as kafkaescli.lib.middleware.MiddlewarePipeline #aliceblue {
  callback
  extra_kwargs : Optional[Dict[str, Any]]
  middleware : List[models.Middleware]
  execute(bundle: Bundle) -> Bundle
}
class "Producer" as kafkaescli.lib.kafka.Producer #aliceblue {
  bootstrap_servers : str
  key : Optional[bytes]
  partition : int
  topic : str
  value : bytes
  execute() -> ProducerPayload
}
class "SyncHookCallback" as kafkaescli.lib.middleware.SyncHookCallback #aliceblue {
}
class "WebhookHandler" as kafkaescli.lib.webhook.WebhookHandler #aliceblue {
  webhook : Optional[str]
  context()
  execute(payload: ConsumerPayload) -> None
}
kafkaescli.lib.commands.AsyncCommand --|> kafkaescli.lib.commands.CommandInterface
kafkaescli.lib.commands.Command --|> kafkaescli.lib.commands.CommandInterface
aiohttp.client.ClientSession --* kafkaescli.lib.webhook.WebhookHandler : _session
aiokafka.consumer.consumer.AIOKafkaConsumer --* kafkaescli.lib.kafka.Consumer : _consumer
aiokafka.producer.producer.AIOKafkaProducer --* kafkaescli.lib.kafka.Producer : _producer
kafkaescli.domain.models.MiddlewareHook --* kafkaescli.lib.middleware.MiddlewarePipeline : callback
@enduml
